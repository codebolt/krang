FANN_FLO_2.1
num_layers=3
learning_rate=0.300000
connection_rate=1.000000
network_type=0
learning_momentum=0.500000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=13 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (9, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.05317610901745360000e-001) (1, -3.81029944076367660000e-002) (2, 1.63591615933081050000e-002) (3, 3.32314918467276000000e-002) (4, -2.04258267358702800000e-004) (5, 3.38931498007447130000e-003) (6, 4.56973139823226850000e-002) (7, 3.31413199144787660000e-002) (8, 4.34488834330155890000e-003) (9, 3.99747782735079970000e-002) (10, 2.35198973370900270000e-002) (11, -4.03848499286730760000e-002) (12, 1.03613463123153700000e-002) (0, -9.74959980863548540000e-002) (1, -3.07136388322433830000e-002) (2, 1.58664599118368280000e-002) (3, 3.77287813994861070000e-002) (4, 2.83625700040295890000e-003) (5, -4.43582512747738890000e-003) (6, 5.15433636232700550000e-002) (7, 1.99523565814627300000e-002) (8, 4.30530337114341800000e-003) (9, 3.53567796947314580000e-002) (10, 2.32376355131689580000e-002) (11, -2.96943124759475600000e-002) (12, 1.11997581081720080000e-002) (0, -1.22405637759126500000e-001) (1, -3.46741775812303350000e-002) (2, 1.46205545920906850000e-002) (3, 4.10770431166145240000e-002) (4, 9.28892438950677280000e-003) (5, 1.29110775735367750000e-004) (6, 6.14655575841576290000e-002) (7, 2.85320911061863470000e-002) (8, 3.36401975574395370000e-003) (9, 4.31662690655962650000e-002) (10, 3.43446236798987530000e-002) (11, -4.00805812792761900000e-002) (12, 1.70008642924630170000e-002) (0, -3.57106366619701980000e-002) (1, -7.21205999869676750000e-003) (2, 2.04779260904343830000e-002) (3, 1.89762456895672840000e-002) (4, 8.76326988101817570000e-003) (5, 6.43074467446557930000e-003) (6, 3.61434011692275130000e-002) (7, 2.65488436840369410000e-002) (8, 1.43962196615168660000e-002) (9, 2.32990896539032630000e-002) (10, 2.43963821490912440000e-002) (11, -1.21595744804939960000e-002) (12, 6.19132343403118460000e-003) (0, -5.85312149275310430000e-002) (1, -2.00592620112481600000e-002) (2, 2.12770686277103560000e-002) (3, 2.43708654420239680000e-002) (4, 1.22026893973093430000e-002) (5, 4.20710956915883170000e-003) (6, 3.58555355847505270000e-002) (7, 3.04000950149217600000e-002) (8, 7.51880549214209610000e-003) (9, 3.42640540047243690000e-002) (10, 1.89651173589168750000e-002) (11, -1.40930451537383800000e-002) (12, 8.69075086933463260000e-003) (0, -4.38983669315029180000e-002) (1, -1.13526604760652430000e-002) (2, 2.24386518119221270000e-002) (3, 1.58341243240751470000e-002) (4, 7.51568196909021490000e-003) (5, 7.66984872194918960000e-003) (6, 3.29613143105469890000e-002) (7, 1.38457422871986980000e-002) (8, 1.04508117008366070000e-002) (9, 2.86681906730139670000e-002) (10, 1.97244247609728170000e-002) (11, -9.44796538504536890000e-004) (12, 8.49100790290816320000e-003) (0, -1.78781170194158900000e-001) (1, -5.42610831857065140000e-002) (2, 1.35191832427231270000e-002) (3, 4.16089096376836070000e-002) (4, 4.59479028650950410000e-003) (5, -7.84746145241137750000e-003) (6, 7.59590267728037370000e-002) (7, 3.67802900356929630000e-002) (8, 5.36266992042773620000e-003) (9, 6.07142000634692400000e-002) (10, 3.20756768414934000000e-002) (11, -6.08927850459564170000e-002) (12, 1.62330386158162780000e-002) (0, -5.11565575920948020000e-002) (1, -1.42107987266077240000e-002) (2, 8.50192509722399320000e-003) (3, 2.70508285795201460000e-002) (4, 2.22775016237553420000e-003) (5, 2.97743407171395570000e-003) (6, 3.40002815252384220000e-002) (7, 2.07112452581115080000e-002) (8, 2.81341515640293630000e-003) (9, 3.06658762597407690000e-002) (10, 2.16280468797306650000e-002) (11, -4.98372010345794500000e-003) (12, 9.06564211586805180000e-003) (13, 8.69121264442919840000e-002) (14, 7.94110912637098210000e-002) (15, 9.88896933949660190000e-002) (16, 3.29904453627179810000e-002) (17, 5.02418967220932410000e-002) (18, 3.62523226828341890000e-002) (19, 1.42230167853599460000e-001) (20, 4.22537679183399020000e-002) (21, 4.04248371277423700000e-002) 
