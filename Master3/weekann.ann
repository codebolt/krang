FANN_FLO_2.1
num_layers=3
learning_rate=0.150000
connection_rate=1.000000
network_type=0
learning_momentum=0.500000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=11 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.28728924097671320000e-002) (1, 9.85174747686125460000e-003) (2, 1.70450199695963940000e-002) (3, 2.02982524323327240000e-002) (4, 1.56662665596345100000e-002) (5, 1.36815957601696460000e-002) (6, 3.12372712318708980000e-002) (7, 1.28861823376583590000e-002) (8, 9.88971251611888220000e-003) (9, -9.87819622020114610000e-003) (10, 1.10555335799895120000e-002) (0, 4.24824101078414680000e-003) (1, 1.39129132287987150000e-002) (2, 9.70071840433084710000e-003) (3, 8.92793797328118520000e-003) (4, 1.83156642608217830000e-002) (5, 4.86540141201409830000e-003) (6, 2.47657762312307180000e-002) (7, 7.32726381007307250000e-003) (8, 4.45545582422334200000e-003) (9, 2.23917898801452970000e-003) (10, 1.17995312950981700000e-002) (0, 1.26983655799894100000e-002) (1, 1.37631063711960920000e-002) (2, 1.76245585922377020000e-002) (3, 1.61059651687928650000e-002) (4, 1.25412009442996150000e-002) (5, 7.25204059101594660000e-003) (6, 3.12266912958467680000e-002) (7, 1.39957783023708580000e-002) (8, 1.15354162518507800000e-002) (9, -1.69070312500689440000e-002) (10, 3.82786624046522850000e-003) (0, 9.68125256234724990000e-003) (1, 1.03574387853629520000e-002) (2, 1.00154802011537050000e-002) (3, 2.14459565020160250000e-002) (4, 1.32706604746768760000e-002) (5, 1.01199319722971650000e-002) (6, 2.42014453009188400000e-002) (7, 1.43811117773500460000e-002) (8, 1.60308656981028340000e-002) (9, -6.77497758377522840000e-003) (10, 1.89195200353099140000e-003) (0, 2.62655633628561870000e-003) (1, 1.08328046089651380000e-002) (2, 1.22770260888424660000e-002) (3, 2.01212960917333190000e-002) (4, 7.14420276119421060000e-003) (5, 8.99542538719495280000e-003) (6, 1.20270882355298070000e-002) (7, 1.39124895373641450000e-002) (8, 1.55843312886338350000e-002) (9, 1.54488271935314200000e-002) (10, 1.28382553867269400000e-002) (0, 2.77854792716123590000e-003) (1, 8.30140296989952120000e-003) (2, 1.67732576660781820000e-002) (3, 1.09023797277120610000e-002) (4, 9.63861387096389380000e-003) (5, 1.05919972527397830000e-002) (6, 2.83228586796690810000e-002) (7, 1.17907228658004700000e-002) (8, 9.64650110031240950000e-003) (9, -3.08811321124715400000e-004) (10, 9.18734166318669470000e-003) (0, 7.82156796910048020000e-003) (1, 6.60505555550015050000e-003) (2, 1.28139631754801060000e-002) (3, 1.53287409719195260000e-002) (4, 9.10492180692723180000e-003) (5, 1.03548456137504010000e-002) (6, 2.89598699469817560000e-002) (7, 1.32599422420397690000e-002) (8, 1.66930945306706900000e-002) (9, -8.57688005382382070000e-003) (10, 2.50591187929724130000e-003) (11, 2.46041238500056130000e-002) (12, 1.22186748557603210000e-002) (13, 2.90959144471475140000e-002) (14, 1.84675559064890610000e-002) (15, -3.36596447425119030000e-003) (16, 1.47388814245929620000e-002) (17, 1.91383328314224320000e-002) (18, 1.66709347904301470000e-002) 
