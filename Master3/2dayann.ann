FANN_FLO_2.1
num_layers=3
learning_rate=0.200000
connection_rate=1.000000
network_type=0
learning_momentum=0.400000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=13 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (13, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (9, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -2.52668002846728850000e-002) (1, -4.38450085226550590000e-003) (2, 2.12408922096491980000e-002) (3, 1.39746186259762080000e-002) (4, 1.69652709016660140000e-002) (5, 9.96827138457360120000e-003) (6, 3.38458652160384820000e-002) (7, 7.17995168854187580000e-003) (8, 1.35356796118635790000e-002) (9, 2.32005122000346380000e-002) (10, 1.88846853774292010000e-002) (11, 9.79685907748080630000e-003) (12, 5.43143338003273560000e-003) (0, -4.08176175068356020000e-002) (1, -1.00555710835950470000e-002) (2, 1.07330604467181550000e-002) (3, 2.43854694671056530000e-002) (4, 1.98494833113891720000e-002) (5, 1.45125207312048680000e-002) (6, 3.33332576362139890000e-002) (7, 1.35606975555449330000e-002) (8, 1.23673124300622720000e-002) (9, 2.27374309281027120000e-002) (10, 9.80503376862559490000e-003) (11, 6.21573099429906930000e-003) (12, 1.51549003546414090000e-002) (0, -7.76388588763962520000e-002) (1, -2.93627701652585980000e-002) (2, 1.80852960913658880000e-002) (3, 2.65834952139046910000e-002) (4, 2.54274789442853580000e-002) (5, 4.14525981878441970000e-003) (6, 5.25321893579420020000e-002) (7, 1.48589926531924170000e-002) (8, 7.11671236761384110000e-003) (9, 2.68526496443423770000e-002) (10, 1.56885355703772350000e-002) (11, -4.28074261291112870000e-003) (12, 1.79538481407926570000e-002) (0, -7.49439255561592520000e-002) (1, -3.66201196031265000000e-002) (2, 1.79347018304197620000e-002) (3, 1.94863286116255080000e-002) (4, 3.07004187814545090000e-002) (5, 1.90484630989067930000e-003) (6, 4.18617092719413070000e-002) (7, 9.07709993287472010000e-003) (8, 2.95080783718021420000e-003) (9, 2.30658929004843480000e-002) (10, 2.58372449315677180000e-002) (11, -3.93165481221764920000e-004) (12, 1.16081560443861450000e-002) (0, -1.78018135220753770000e-001) (1, -7.05320824981679470000e-002) (2, 3.50904559717837780000e-002) (3, 3.57673519712670280000e-002) (4, 4.32365089455660910000e-002) (5, -6.19702097757885420000e-004) (6, 7.85792574313955720000e-002) (7, 1.95844007035424240000e-002) (8, -2.79931937312813750000e-003) (9, 3.23630315640326470000e-002) (10, 2.67886177740842350000e-002) (11, -1.58460851792365800000e-002) (12, 1.95384243392243660000e-002) (0, -1.80901572451451500000e-001) (1, -7.71303191509582630000e-002) (2, 3.52601191373209320000e-002) (3, 3.92252555457464620000e-002) (4, 4.79423243360869240000e-002) (5, -1.86956247836968950000e-003) (6, 9.02127907240127260000e-002) (7, 1.20424234762308070000e-002) (8, -1.81892540462181750000e-003) (9, 4.09766872304227480000e-002) (10, 3.55966028117307780000e-002) (11, -1.63053298619444290000e-002) (12, 1.07368397143199020000e-002) (0, -3.51970535021709640000e-003) (1, -6.54105558997187830000e-003) (2, 1.52155878573286380000e-002) (3, 2.02555241164730640000e-002) (4, 1.10629964070973390000e-002) (5, 3.75539989145900600000e-003) (6, 2.34811828391310800000e-002) (7, 2.18710817180786940000e-002) (8, 1.48866287022751830000e-002) (9, 9.89380556354826040000e-003) (10, 1.00645485943734490000e-002) (11, 1.30485303521628800000e-002) (12, 8.14130338929457650000e-003) (0, 1.21801681188455910000e-002) (1, 2.10007393234654770000e-003) (2, 1.06478515450020020000e-002) (3, 1.73193319828182370000e-002) (4, 2.19141424106923390000e-002) (5, 1.97256249087355870000e-002) (6, 1.31274364050265810000e-002) (7, 1.92153689439159520000e-002) (8, 8.73211981577334640000e-003) (9, 9.29706664824133350000e-003) (10, 8.22143171187973870000e-003) (11, 7.48061765428119800000e-003) (12, 5.62371549673007680000e-003) (13, 3.04505794544582820000e-002) (14, 4.31874758596713230000e-002) (15, 8.21086100450131030000e-002) (16, 7.90918010915996890000e-002) (17, 1.75587464813317630000e-001) (18, 1.83100066158864370000e-001) (19, 1.20479087102046390000e-002) (20, -4.31485296215226250000e-003) (21, 4.18934716118703200000e-002) 
