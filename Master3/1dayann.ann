FANN_FLO_2.1
num_layers=3
learning_rate=0.300000
connection_rate=1.000000
network_type=0
learning_momentum=0.500000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=11 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (11, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -2.00796484201120550000e-002) (1, -2.97120654973099560000e-002) (2, -4.37489171616344610000e-002) (3, 2.05742259836827030000e-002) (4, 2.10048799034415480000e-002) (5, 1.04957999900284000000e-002) (6, 1.37044852907573720000e-002) (7, 1.31427469118042300000e-002) (8, 9.55879736035694650000e-003) (9, 1.84785932220348020000e-002) (10, 9.38902037793216790000e-003) (0, -4.48603373162654450000e-002) (1, -3.97737385206224440000e-002) (2, -6.04810160789889320000e-002) (3, 1.68628302236909880000e-002) (4, 2.54310255093325480000e-002) (5, 1.81256124751642510000e-002) (6, 7.92345645428153490000e-003) (7, 2.00546943675366550000e-002) (8, 1.20488621257615370000e-002) (9, 2.51833304858057120000e-002) (10, 1.46661211186806700000e-002) (0, 3.53931261928064410000e-002) (1, 2.47830755433881380000e-002) (2, 3.77140263333291050000e-002) (3, 1.71829604211636770000e-002) (4, 8.61760588589375460000e-003) (5, 9.50486780392229520000e-003) (6, 1.84206631828813320000e-002) (7, 1.29397844172657200000e-002) (8, 1.38400226780840390000e-002) (9, 5.85928949608688440000e-003) (10, -8.76162608040287510000e-003) (0, 3.21974962129551320000e-002) (1, 1.89449429295925450000e-002) (2, 2.92843616963003870000e-002) (3, 1.18155107748157890000e-002) (4, 1.32688775125817290000e-002) (5, 8.91752196032167750000e-003) (6, 1.63803282829353240000e-002) (7, 1.50493338849811710000e-002) (8, 1.34087696064237650000e-002) (9, 1.44382813024615980000e-002) (10, -6.10422675699531360000e-003) (0, -1.06361051079622470000e-002) (1, -9.97188010899144330000e-003) (2, -2.42700987309098890000e-002) (3, 1.48488202430870010000e-002) (4, 2.06290065157424260000e-002) (5, 1.55557216038697060000e-002) (6, 1.80290153275893570000e-002) (7, 1.47859249599129690000e-002) (8, 8.98922042581073310000e-003) (9, 1.39155643570976640000e-002) (10, 5.82194988424164940000e-003) (0, -1.02662324412026320000e-001) (1, -9.23245274990547450000e-002) (2, -1.44762350930819320000e-001) (3, 1.96366740843258930000e-002) (4, 4.05785434543084470000e-002) (5, 1.84512908314378060000e-002) (6, 6.88230058356937170000e-003) (7, 1.17748402456447040000e-002) (8, 1.24336775754304210000e-002) (9, 3.05128612142225270000e-002) (10, 2.81455217990637520000e-002) (0, -6.66813147044586360000e-002) (1, -5.62216332783690500000e-002) (2, -1.02147003718066800000e-001) (3, 2.20339249352650840000e-002) (4, 2.93142579717390430000e-002) (5, 2.09140986317161510000e-002) (6, 7.06327625021004650000e-003) (7, 1.23373896900989530000e-002) (8, 1.36982861991731920000e-002) (9, 2.11585439317887450000e-002) (10, 2.06742272581780730000e-002) (11, 4.64505463255321380000e-002) (12, 7.17139461995473790000e-002) (13, -4.89629966057294470000e-002) (14, -4.01849903380737320000e-002) (15, 2.30878796649857840000e-002) (16, 1.69240188567786610000e-001) (17, 1.12880648758620820000e-001) (18, 1.14386706502065340000e-002) 
